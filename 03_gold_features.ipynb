{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22530c19-a117-40f4-bdea-935b99a7ac0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Gold v1 — Features e Análises (IBOV, SP500, DXY)\n",
    "# MAGIC\n",
    "# MAGIC ## Objetivos\n",
    "# MAGIC A partir das tabelas Silver, construir uma camada Gold orientada a análise quantitativa:\n",
    "# MAGIC - Features técnicas do IBOV (retornos, médias, Bollinger, ATR, label)\n",
    "# MAGIC - Retornos diários (IBOV, SP500, DXY) em base alinhada\n",
    "# MAGIC - Correlação global (Pearson) e correlação dinâmica (rolling)\n",
    "# MAGIC - Tabela analítica unificada para exploração e modelagem\n",
    "# MAGIC\n",
    "# MAGIC ## Entradas (Silver)\n",
    "# MAGIC - `mvp_finance.silver_prices_long`\n",
    "# MAGIC - `mvp_finance.silver_prices_wide_aligned` (preferencial)\n",
    "# MAGIC - `mvp_finance.silver_returns_wide` (preferencial)\n",
    "# MAGIC\n",
    "# MAGIC ## Saídas (Gold)\n",
    "# MAGIC - `mvp_finance.gold_ibov_features`\n",
    "# MAGIC - `mvp_finance.gold_retornos_global`\n",
    "# MAGIC - `mvp_finance.gold_corr_ibov_global`\n",
    "# MAGIC - `mvp_finance.gold_analytics_ibov_global`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62c22e9-b50e-4d0f-905d-550145d28600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 0) Imports e contexto\n",
    "# COMMAND ----------\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark.sql(\"USE mvp_finance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "262d8fac-370f-46c9-99ee-04112ac8591b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1) Leitura das tabelas Silver\n",
    "# MAGIC Carregamos as tabelas e validamos rapidamente schemas/amostras.\n",
    "# MAGIC\n",
    "# MAGIC Observação:\n",
    "# MAGIC - `wide_aligned` + `returns_wide` são preferíveis para evitar gaps e “matriz quebrada”.\n",
    "# COMMAND ----------\n",
    "silver_long   = spark.table(\"silver_prices_long\")\n",
    "silver_wide_a = spark.table(\"silver_prices_wide_aligned\")\n",
    "silver_ret    = spark.table(\"silver_returns_wide\")\n",
    "\n",
    "print(\"Schema silver_prices_long:\")\n",
    "silver_long.printSchema()\n",
    "\n",
    "print(\"Schema silver_prices_wide_aligned:\")\n",
    "silver_wide_a.printSchema()\n",
    "\n",
    "print(\"Schema silver_returns_wide:\")\n",
    "silver_ret.printSchema()\n",
    "\n",
    "display(silver_long.limit(5))\n",
    "display(silver_wide_a.limit(5))\n",
    "display(silver_ret.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401ae2de-fb08-4a0b-ac4a-d1b67e9df679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2) GOLD — Features técnicas Multi-Ativo (`gold_asset_features`)\n",
    "# MAGIC\n",
    "# MAGIC **Base:** `silver_prices_long` (todos os símbolos disponíveis)\n",
    "# MAGIC\n",
    "# MAGIC **Features (por símbolo):**\n",
    "# MAGIC - `close_prev`, `retorno`, `retorno_log`\n",
    "# MAGIC - `sma20`, `sma50`\n",
    "# MAGIC - `boll_upper`, `boll_lower` (20 períodos, 2 desvios)\n",
    "# MAGIC - `true_range`, `atr14` (ATR clássico via TR rolling)\n",
    "# MAGIC - `label_direcao` (1 se retorno > 0, senão 0)\n",
    "# MAGIC\n",
    "# MAGIC **Observação de rigor:**\n",
    "# MAGIC ATR usa:\n",
    "# MAGIC `TR = max(high-low, |high-close_prev|, |low-close_prev|)`\n",
    "\n",
    "# COMMAND ----------\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Janela por ativo (símbolo)\n",
    "w_sym = Window.partitionBy(\"symbol\").orderBy(\"timestamp\")\n",
    "\n",
    "features_df = (\n",
    "    silver_long\n",
    "    .orderBy(\"symbol\", \"timestamp\")\n",
    "    .withColumn(\"close_prev\", F.lag(\"close\", 1).over(w_sym))\n",
    "    .withColumn(\"retorno\", (F.col(\"close\") / F.col(\"close_prev\")) - F.lit(1))\n",
    "    .withColumn(\"retorno_log\", F.log(F.col(\"close\") / F.col(\"close_prev\")))\n",
    "    .withColumn(\"sma20\", F.avg(\"close\").over(w_sym.rowsBetween(-19, 0)))\n",
    "    .withColumn(\"sma50\", F.avg(\"close\").over(w_sym.rowsBetween(-49, 0)))\n",
    "    .withColumn(\"std20\", F.stddev(\"close\").over(w_sym.rowsBetween(-19, 0)))\n",
    "    .withColumn(\"boll_upper\", F.col(\"sma20\") + F.lit(2) * F.col(\"std20\"))\n",
    "    .withColumn(\"boll_lower\", F.col(\"sma20\") - F.lit(2) * F.col(\"std20\"))\n",
    ")\n",
    "\n",
    "tr = F.greatest(\n",
    "    (F.col(\"high\") - F.col(\"low\")),\n",
    "    F.abs(F.col(\"high\") - F.lag(\"close\", 1).over(w_sym)),\n",
    "    F.abs(F.col(\"low\")  - F.lag(\"close\", 1).over(w_sym)),\n",
    ")\n",
    "\n",
    "features_df = (\n",
    "    features_df\n",
    "    .withColumn(\"true_range\", tr)\n",
    "    .withColumn(\"atr14\", F.avg(\"true_range\").over(w_sym.rowsBetween(-13, 0)))\n",
    "    .withColumn(\"label_direcao\", F.when(F.col(\"retorno\") > 0, F.lit(1)).otherwise(F.lit(0)))\n",
    ")\n",
    "\n",
    "display(features_df.select(\n",
    "    \"symbol\",\"timestamp\",\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "    \"retorno\",\"retorno_log\",\"sma20\",\"sma50\",\"boll_upper\",\"boll_lower\",\"atr14\",\"label_direcao\"\n",
    ").orderBy(\"symbol\",\"timestamp\").limit(20))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf9cc9c-ac57-4135-929f-6572400c506f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Análise Exploratória das Features — Exemplo com Bitcoin\n",
    "\n",
    "Nesta seção realizamos uma **análise exploratória (EDA)** das features calculadas na camada **Gold**, com o objetivo de **validar visualmente** se os indicadores técnicos construídos fazem sentido do ponto de vista financeiro e estatístico.\n",
    "\n",
    "### Por que escolhemos o Bitcoin?\n",
    "\n",
    "O **Bitcoin foi escolhido de forma intencionalmente aleatória**, apenas como **ativo representativo** para demonstração visual.  \n",
    "Os mesmos cálculos e gráficos **podem ser aplicados a qualquer outro ativo** presente no dataset (`NASDAQ`, `DOWJONES`, `PETROLEO`, `TREASURY10Y`, etc.), pois todas as features foram construídas de forma **genérica e multi-ativo**.\n",
    "\n",
    "O uso do Bitcoin facilita a interpretação visual por apresentar:\n",
    "- Alta volatilidade\n",
    "- Regimes claros de tendência\n",
    "- Movimentos amplos que tornam indicadores técnicos mais evidentes\n",
    "\n",
    "##  Preço + Médias Móveis + Bandas de Bollinger\n",
    "\n",
    "**Objetivo:** \n",
    "\n",
    "Validar se as médias móveis (`SMA20`, `SMA50`) e as Bandas de Bollinger capturam corretamente:\n",
    "- Tendência\n",
    "- Consolidação\n",
    "- Regiões de sobrecompra e sobrevenda\n",
    "\n",
    "**Interpretação esperada:**\n",
    "- Preço acima da SMA20/SMA50 indica tendência de alta\n",
    "- Toques ou rompimentos das bandas sugerem extremos de preço\n",
    "- Estreitamento das bandas indica redução de volatilidade\n",
    "\n",
    "Este gráfico confirma que as bandas e médias acompanham o comportamento do preço de forma coerente.\n",
    "\n",
    "## ATR14 — Volatilidade\n",
    "\n",
    "**Objetivo:** \n",
    "\n",
    "Analisar se o indicador de volatilidade (`ATR14`) reage adequadamente a períodos de estresse e calmaria do mercado.\n",
    "\n",
    "**Interpretação esperada:**\n",
    "- Picos de ATR coincidem com movimentos bruscos do preço\n",
    "- ATR reduzido indica períodos de consolidação\n",
    "- ATR serve como base para gestão de risco e dimensionamento de posição\n",
    "\n",
    "O gráfico mostra claramente a alternância entre regimes de baixa e alta volatilidade.\n",
    "\n",
    "## Retornos Diários\n",
    "\n",
    "**Objetivo:**  \n",
    "\n",
    "Verificar a sanidade estatística dos retornos:\n",
    "- Presença de outliers\n",
    "- Simetria em torno de zero\n",
    "- Estabilidade ao longo do tempo\n",
    "\n",
    "**Interpretação esperada:**\n",
    "- Retornos oscilam em torno de zero\n",
    "- Não há explosões artificiais (sanity check)\n",
    "- Confirma que o cálculo de retorno está correto\n",
    "\n",
    "\n",
    "## Distribuição dos Retornos\n",
    "\n",
    "**Objetivo:**  \n",
    "\n",
    "Avaliar a distribuição estatística dos retornos para futuras aplicações em:\n",
    "- Modelagem estatística\n",
    "- Machine Learning\n",
    "- Gestão de risco\n",
    "\n",
    "**Interpretação esperada:**\n",
    "- Distribuição aproximadamente centrada em zero\n",
    "- Caudas mais longas (característica comum de ativos financeiros)\n",
    "- Não-normalidade esperada (fat tails)\n",
    "\n",
    "Este gráfico confirma que os retornos não seguem uma distribuição perfeitamente normal, o que é consistente com mercados reais.\n",
    "\n",
    "## Retorno vs Label de Direção\n",
    "\n",
    "**Objetivo:** \n",
    "\n",
    "Validar se o `label_direcao` separa corretamente dias positivos e negativos.\n",
    "\n",
    "**Definição do label:**\n",
    "- `label_direcao = 1` → retorno positivo\n",
    "- `label_direcao = 0` → retorno negativo ou zero\n",
    "\n",
    "**Interpretação esperada:**\n",
    "- Retornos associados ao label 1 tendem a ser positivos\n",
    "- Retornos associados ao label 0 tendem a ser negativos\n",
    "- Confirmação de que o label é consistente para uso em ML supervisionado\n",
    "\n",
    "O boxplot mostra uma separação clara entre os grupos, validando a lógica do rótulo.\n",
    "\n",
    "## Conclusão desta etapa\n",
    "\n",
    "Esta análise exploratória confirma que:\n",
    "\n",
    "- As **features técnicas estão corretamente calculadas**\n",
    "- Os indicadores capturam padrões financeiros reais\n",
    "- Os dados estão **prontos para uso em hipóteses, correlação, ML e geração de sinais**\n",
    "\n",
    "A partir deste ponto, o pipeline está preparado para avançar para:\n",
    "- Testes estatísticos formais\n",
    "- Construção de scores determinísticos\n",
    "- Modelos de Machine Learning\n",
    "- Backtests e validação econômica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ff618c-cfdf-43d3-8595-f6983b7c2805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# GOLD — Plot: Preço + SMA + Bollinger (1 ativo)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "symbol = \"BITCOIN\"  # troque livremente\n",
    "\n",
    "pdf = (\n",
    "    features_df\n",
    "    .filter(F.col(\"symbol\") == symbol)\n",
    "    .orderBy(\"timestamp\")\n",
    "    .select(\n",
    "        \"timestamp\",\"close\",\"sma20\",\"sma50\",\"boll_upper\",\"boll_lower\"\n",
    "    )\n",
    "    .dropna()\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(pdf[\"timestamp\"], pdf[\"close\"], label=\"Close\", color=\"black\")\n",
    "plt.plot(pdf[\"timestamp\"], pdf[\"sma20\"], label=\"SMA20\", linestyle=\"--\")\n",
    "plt.plot(pdf[\"timestamp\"], pdf[\"sma50\"], label=\"SMA50\", linestyle=\"--\")\n",
    "plt.fill_between(\n",
    "    pdf[\"timestamp\"],\n",
    "    pdf[\"boll_lower\"],\n",
    "    pdf[\"boll_upper\"],\n",
    "    alpha=0.2,\n",
    "    label=\"Bollinger\"\n",
    ")\n",
    "\n",
    "plt.title(f\"{symbol} — Preço + Médias + Bollinger\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42063924-9398-4e60-b9c7-a83b66cd47a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# GOLD — Plot: ATR14\n",
    "\n",
    "pdf_atr = (\n",
    "    features_df\n",
    "    .filter(F.col(\"symbol\") == symbol)\n",
    "    .orderBy(\"timestamp\")\n",
    "    .select(\"timestamp\",\"atr14\")\n",
    "    .dropna()\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(pdf_atr[\"timestamp\"], pdf_atr[\"atr14\"], color=\"darkred\")\n",
    "plt.title(f\"{symbol} — ATR14 (Volatilidade)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a9f138-c827-43b8-bf1b-914d6de5388c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# GOLD — Plot: Retornos\n",
    "\n",
    "pdf_ret = (\n",
    "    features_df\n",
    "    .filter(F.col(\"symbol\") == symbol)\n",
    "    .orderBy(\"timestamp\")\n",
    "    .select(\"timestamp\",\"retorno\")\n",
    "    .dropna()\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(pdf_ret[\"timestamp\"], pdf_ret[\"retorno\"], alpha=0.6)\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.title(f\"{symbol} — Retornos Diários\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b830818f-c5ce-4811-a38b-dbad64007ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# GOLD — Histograma de retornos\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(pdf_ret[\"retorno\"], bins=100)\n",
    "plt.title(f\"{symbol} — Distribuição dos Retornos\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c48151-868e-4443-b3cb-9d9fe238e37d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# GOLD — Label vs Retorno\n",
    "\n",
    "pdf_lbl = (\n",
    "    features_df\n",
    "    .filter(F.col(\"symbol\") == symbol)\n",
    "    .orderBy(\"timestamp\")\n",
    "    .select(\"retorno\",\"label_direcao\")\n",
    "    .dropna()\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "pdf_lbl.boxplot(by=\"label_direcao\", column=\"retorno\", grid=True)\n",
    "plt.title(f\"{symbol} — Retorno vs Label Direção\")\n",
    "plt.suptitle(\"\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe2ac45-dcef-4289-a46a-0dd9502eb0d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.1) Persistência — `gold_asset_features`\n",
    "# MAGIC Tabela única com features por símbolo, pronta para EDA e modelagem.\n",
    "# COMMAND ----------\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold_asset_features\")\n",
    "\n",
    "features_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_asset_features\")\n",
    "\n",
    "print(\"✅ Tabela criada: mvp_finance.gold_asset_features\")\n",
    "spark.sql(\"SELECT symbol, COUNT(*) AS qtde FROM gold_asset_features GROUP BY symbol ORDER BY qtde DESC\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5a5afc-b820-436a-9fd7-0966c7b9f451",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765562732609}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3) GOLD — Retornos alinhados (multi-ativo) (`gold_retornos_global`)\n",
    "# MAGIC\n",
    "# MAGIC **Base:** `silver_returns_wide`\n",
    "# MAGIC - contém `trade_date`, preços, colunas `*_prev` e retornos `*_ret`\n",
    "# MAGIC - é a base canônica para correlação, heatmap e validação de hipóteses\n",
    "# MAGIC\n",
    "# MAGIC **Nota:**\n",
    "# MAGIC Apesar do nome `gold_retornos_global`, a tabela passa a conter retornos **multi-ativo**.\n",
    "# COMMAND ----------\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# silver_ret == silver_returns_wide (já carregado antes)\n",
    "df = silver_ret\n",
    "\n",
    "if \"trade_date\" not in df.columns:\n",
    "    raise RuntimeError(\"[GOLD RET] silver_returns_wide não possui a coluna trade_date.\")\n",
    "\n",
    "# colunas de preço (base): tudo que não é trade_date e não termina com _prev/_ret\n",
    "price_cols = [\n",
    "    c for c in df.columns\n",
    "    if c != \"trade_date\" and not c.endswith(\"_prev\") and not c.endswith(\"_ret\")\n",
    "]\n",
    "\n",
    "prev_cols = [c for c in df.columns if c.endswith(\"_prev\")]\n",
    "ret_cols  = [c for c in df.columns if c.endswith(\"_ret\")]\n",
    "\n",
    "# ordena colunas para ficar estável\n",
    "price_cols = sorted(price_cols)\n",
    "prev_cols  = sorted(prev_cols)\n",
    "ret_cols   = sorted(ret_cols)\n",
    "\n",
    "gold_ret = (\n",
    "    df.select(\"trade_date\", *price_cols, *prev_cols, *ret_cols)\n",
    "      .orderBy(\"trade_date\")\n",
    ")\n",
    "\n",
    "display(gold_ret.limit(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold_retornos_global\")\n",
    "\n",
    "gold_ret.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_retornos_global\")\n",
    "\n",
    "print(\"✅ Tabela criada: mvp_finance.gold_retornos_global\")\n",
    "spark.sql(\"SELECT COUNT(*) AS qtde FROM gold_retornos_global\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0f9dbc-5a8c-4321-bec2-9f1a4ec27bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.1) Quality Gate — `gold_retornos_global` (multi-ativo)\n",
    "# MAGIC Validações:\n",
    "# MAGIC - `trade_date` não nulo e único\n",
    "# MAGIC - retornos `*_ret` finitos (sem NaN / sem explosões típicas de divisão por zero)\n",
    "# MAGIC - limites conservadores para alertar outliers\n",
    "# MAGIC\n",
    "# MAGIC Observação:\n",
    "# MAGIC - linhas iniciais podem ter NULL (por conta do `lag`), isso é normal.\n",
    "# COMMAND ----------\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1) trade_date não nulo\n",
    "null_td = gold_ret.filter(F.col(\"trade_date\").isNull()).count()\n",
    "if null_td > 0:\n",
    "    raise RuntimeError(f\"[GOLD RET] trade_date NULL (qtde={null_td})\")\n",
    "\n",
    "# 2) trade_date único\n",
    "dups_td = gold_ret.groupBy(\"trade_date\").count().filter(F.col(\"count\") > 1).count()\n",
    "if dups_td > 0:\n",
    "    raise RuntimeError(f\"[GOLD RET] duplicidade por trade_date (qtde={dups_td})\")\n",
    "\n",
    "# 3) identifica colunas de retorno\n",
    "ret_cols = [c for c in gold_ret.columns if c.endswith(\"_ret\")]\n",
    "if not ret_cols:\n",
    "    raise RuntimeError(\"[GOLD RET] Nenhuma coluna *_ret encontrada para validação.\")\n",
    "\n",
    "# mantemos apenas linhas com pelo menos 1 retorno não-nulo (para não matar o dataset no início)\n",
    "ret_clean = gold_ret.filter(\n",
    "    F.greatest(*[F.col(c).isNotNull().cast(\"int\") for c in ret_cols]) == 1\n",
    ")\n",
    "\n",
    "# 4) NaN check\n",
    "nan_counts = (\n",
    "    ret_clean\n",
    "    .select(*[F.sum(F.isnan(F.col(c)).cast(\"int\")).alias(c) for c in ret_cols])\n",
    "    .collect()[0]\n",
    "    .asDict()\n",
    ")\n",
    "\n",
    "total_nan = sum(nan_counts.values())\n",
    "if total_nan > 0:\n",
    "    print(\"[GOLD RET] NaNs detectados (por coluna):\")\n",
    "    for k, v in sorted(nan_counts.items(), key=lambda kv: kv[1], reverse=True):\n",
    "        if v > 0:\n",
    "            print(f\"  - {k}: {v}\")\n",
    "    raise RuntimeError(f\"[GOLD RET] Existem NaNs em retornos (total={total_nan}).\")\n",
    "\n",
    "# 5) “Inf-like” guard (retornos absurdamente altos geralmente são dado ruim)\n",
    "INF_GUARD = 10.0  # 1000% (guarda técnica)\n",
    "inf_like = ret_clean.filter(\n",
    "    F.greatest(*[F.abs(F.col(c)) for c in ret_cols]) > F.lit(INF_GUARD)\n",
    ")\n",
    "inf_cnt = inf_like.count()\n",
    "if inf_cnt > 0:\n",
    "    display(inf_like.select(\"trade_date\", *ret_cols).orderBy(\"trade_date\"))\n",
    "    raise RuntimeError(f\"[GOLD RET] Retornos com |ret| > {INF_GUARD} (possível Inf/divisão por zero). qtde={inf_cnt}\")\n",
    "\n",
    "# 6) sanity check conservador\n",
    "abs_limit = 0.50\n",
    "bad = ret_clean.filter(\n",
    "    F.greatest(*[F.abs(F.col(c)) for c in ret_cols]) > F.lit(abs_limit)\n",
    ")\n",
    "\n",
    "bad_cnt = bad.count()\n",
    "if bad_cnt > 0:\n",
    "    bad_ranked = bad.withColumn(\n",
    "        \"_max_abs_ret\",\n",
    "        F.greatest(*[F.abs(F.col(c)) for c in ret_cols])\n",
    "    ).orderBy(F.col(\"_max_abs_ret\").desc())\n",
    "\n",
    "    display(bad_ranked.select(\"trade_date\", \"_max_abs_ret\", *ret_cols).limit(200))\n",
    "    raise RuntimeError(f\"[GOLD RET] retornos fora do limite ±{abs_limit*100:.0f}% (qtde={bad_cnt})\")\n",
    "\n",
    "print(\"[GOLD RET] OK\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a0569f-2fb3-4bb7-b937-4795a6cd7027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.2) Persistência — `gold_retornos_global`\n",
    "# COMMAND ----------\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold_retornos_global\")\n",
    "\n",
    "gold_ret.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_retornos_global\")\n",
    "\n",
    "print(\"✅ Tabela criada: mvp_finance.gold_retornos_global\")\n",
    "spark.sql(\"SELECT COUNT(*) AS qtde FROM gold_retornos_global\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d48c2f36-576d-4646-9177-0d4926b22db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4) GOLD — Correlações (global e dinâmica) (multi-ativo)\n",
    "# MAGIC\n",
    "# MAGIC Produzimos:\n",
    "# MAGIC - **Correlação global (Pearson)** no período para **todos os pares** de ativos (retornos)\n",
    "# MAGIC - **Correlação rolling 21d** para **todos os pares** (evidência de mudança estrutural)\n",
    "# MAGIC\n",
    "# MAGIC Saídas (recomendado):\n",
    "# MAGIC - `gold_corr_pairs_global` (1 linha por par no período inteiro)\n",
    "# MAGIC - `gold_corr_pairs_rolling_21d` (série temporal por par)\n",
    "# COMMAND ----------\n",
    "import pyspark.sql.functions as F\n",
    "from itertools import combinations\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ret_cols: todas as colunas *_ret presentes em gold_ret\n",
    "ret_cols = [c for c in gold_ret.columns if c.endswith(\"_ret\")]\n",
    "if not ret_cols:\n",
    "    raise RuntimeError(\"[GOLD CORR] Nenhuma coluna *_ret encontrada em gold_ret.\")\n",
    "\n",
    "# nomes \"limpos\" dos ativos (remove _ret)\n",
    "assets = [c.replace(\"_ret\", \"\") for c in ret_cols]\n",
    "\n",
    "print(\"[GOLD CORR] ativos detectados:\", assets)\n",
    "# COMMAND ----------\n",
    "# Gera expressões corr(col_i, col_j) para todos os pares\n",
    "pair_exprs = []\n",
    "pairs = list(combinations(ret_cols, 2))\n",
    "\n",
    "for a, b in pairs:\n",
    "    a_name = a.replace(\"_ret\", \"\")\n",
    "    b_name = b.replace(\"_ret\", \"\")\n",
    "    pair_exprs.append(F.corr(F.col(a), F.col(b)).alias(f\"corr__{a_name}__{b_name}\"))\n",
    "\n",
    "# Uma linha com todas as correlações (wide)\n",
    "corr_wide = gold_ret.select(*pair_exprs)\n",
    "\n",
    "display(corr_wide)\n",
    "# COMMAND ----------\n",
    "# Traz para driver (pouquíssimo dado: só 1 linha)\n",
    "row = corr_wide.collect()[0].asDict()\n",
    "\n",
    "corr_rows = []\n",
    "for k, v in row.items():\n",
    "    # k = \"corr__A__B\"\n",
    "    _, a, b = k.split(\"__\")\n",
    "    corr_rows.append((a, b, float(v) if v is not None else None))\n",
    "\n",
    "corr_global_pairs = spark.createDataFrame(corr_rows, [\"asset_a\", \"asset_b\", \"corr_pearson\"])\n",
    "\n",
    "display(corr_global_pairs.orderBy(F.col(\"corr_pearson\").desc_nulls_last()))\n",
    "# COMMAND ----------\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold_corr_pairs_global\")\n",
    "corr_global_pairs.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_corr_pairs_global\")\n",
    "print(\"✅ Tabela criada: mvp_finance.gold_corr_pairs_global\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c94060-ebe0-44cd-8c7d-4813427b05e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "w21 = Window.orderBy(\"trade_date\").rowsBetween(-20, 0)\n",
    "\n",
    "def rolling_corr_safe(df, x, y, out, min_points=10, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Rolling corr robusta:\n",
    "    - exige min_points válidos na janela\n",
    "    - evita divisão por zero (stddev=0)\n",
    "    - clampa pequenas violações numéricas para [-1, 1]\n",
    "    \"\"\"\n",
    "    xcol = F.col(x)\n",
    "    ycol = F.col(y)\n",
    "\n",
    "    # quantos pares (x,y) não-nulos na janela\n",
    "    n = F.count(xcol * ycol).over(w21)\n",
    "\n",
    "    ex  = F.avg(xcol).over(w21)\n",
    "    ey  = F.avg(ycol).over(w21)\n",
    "    exy = F.avg(xcol * ycol).over(w21)\n",
    "\n",
    "    cov = exy - (ex * ey)\n",
    "    sx  = F.stddev(xcol).over(w21)\n",
    "    sy  = F.stddev(ycol).over(w21)\n",
    "\n",
    "    denom = sx * sy\n",
    "    raw = cov / denom\n",
    "\n",
    "    # validade: pontos suficientes e denom > 0\n",
    "    valid = (n >= F.lit(min_points)) & denom.isNotNull() & (denom > F.lit(0))\n",
    "\n",
    "    # clamp para tolerar artefatos numéricos mínimos\n",
    "    clamped = (\n",
    "        F.when(raw > F.lit(1) + F.lit(eps), F.lit(1.0))\n",
    "         .when(raw < F.lit(-1) - F.lit(eps), F.lit(-1.0))\n",
    "         .otherwise(raw)\n",
    "    )\n",
    "\n",
    "    return df.withColumn(out, F.when(valid, clamped).otherwise(F.lit(None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784f6c4d-fcca-4545-ab71-a36af27ed419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "ret_cols = [c for c in gold_ret.columns if c.endswith(\"_ret\")]\n",
    "pairs = list(combinations(ret_cols, 2))\n",
    "\n",
    "df_roll = (\n",
    "    gold_ret\n",
    "    .select(\"trade_date\", *ret_cols)\n",
    "    .orderBy(\"trade_date\")\n",
    ")\n",
    "\n",
    "out_cols = []\n",
    "for a, b in pairs:\n",
    "    a_name = a.replace(\"_ret\", \"\")\n",
    "    b_name = b.replace(\"_ret\", \"\")\n",
    "    out = f\"corr__{a_name}__{b_name}__21d\"\n",
    "    df_roll = rolling_corr_safe(df_roll, a, b, out, min_points=10)\n",
    "    out_cols.append(out)\n",
    "\n",
    "# long format: (trade_date, asset_a, asset_b, corr_21d)\n",
    "expr_stack = []\n",
    "for out in out_cols:\n",
    "    _, a, b, _ = out.split(\"__\")\n",
    "    expr_stack.append(\n",
    "        F.struct(\n",
    "            F.lit(a).alias(\"asset_a\"),\n",
    "            F.lit(b).alias(\"asset_b\"),\n",
    "            F.col(out).alias(\"corr_21d\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "corr_roll_long = (\n",
    "    df_roll\n",
    "    .select(\"trade_date\", F.explode(F.array(*expr_stack)).alias(\"x\"))\n",
    "    .select(\"trade_date\", \"x.asset_a\", \"x.asset_b\", \"x.corr_21d\")\n",
    ")\n",
    "\n",
    "display(corr_roll_long.orderBy(\"trade_date\").limit(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e428f3-e988-4218-80f0-7bff79240379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tolerância numérica\n",
    "TOL = 1e-6\n",
    "\n",
    "bad_corr = (\n",
    "    corr_roll_long\n",
    "    .dropna(subset=[\"corr_21d\"])\n",
    "    .filter((F.col(\"corr_21d\") > F.lit(1 + TOL)) | (F.col(\"corr_21d\") < F.lit(-1 - TOL)))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "if bad_corr > 0:\n",
    "    display(\n",
    "        corr_roll_long\n",
    "        .dropna(subset=[\"corr_21d\"])\n",
    "        .filter((F.col(\"corr_21d\") > F.lit(1 + TOL)) | (F.col(\"corr_21d\") < F.lit(-1 - TOL)))\n",
    "        .orderBy(\"trade_date\")\n",
    "    )\n",
    "    raise RuntimeError(f\"[GOLD CORR ROLL] valores fora de [-1,1] além da tolerância (qtde={bad_corr})\")\n",
    "\n",
    "print(\"[GOLD CORR ROLL] OK — robusto com clamp/guard\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS mvp_finance.gold_corr_pairs_rolling_21d\")\n",
    "corr_roll_long.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"mvp_finance.gold_corr_pairs_rolling_21d\")\n",
    "print(\"✅ Tabela criada: mvp_finance.gold_corr_pairs_rolling_21d\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05cc7b31-9b89-426b-b047-1ca38cc2f1b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.1) Correlação dinâmica (rolling 21d) — multi-ativo\n",
    "# MAGIC Implementação via janelas (fórmula rolling):\n",
    "# MAGIC\n",
    "# MAGIC corr(X,Y) = cov(X,Y) / (std(X)*std(Y))\n",
    "# MAGIC cov = E[XY] - E[X]E[Y]\n",
    "# MAGIC\n",
    "# MAGIC **Janela:** 21 pregões (ajustável).\n",
    "# MAGIC\n",
    "# MAGIC **Saída (recomendada):**\n",
    "# MAGIC - `gold_corr_pairs_rolling_21d` (formato longo: trade_date + par + correlação)\n",
    "# COMMAND ----------\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from itertools import combinations\n",
    "\n",
    "# Janela rolling: 21 (inclui o dia atual)\n",
    "w21 = Window.orderBy(\"trade_date\").rowsBetween(-20, 0)\n",
    "\n",
    "def rolling_corr(df, x, y, out):\n",
    "    ex  = F.avg(F.col(x)).over(w21)\n",
    "    ey  = F.avg(F.col(y)).over(w21)\n",
    "    exy = F.avg((F.col(x) * F.col(y))).over(w21)\n",
    "    cov = exy - (ex * ey)\n",
    "    sx  = F.stddev(F.col(x)).over(w21)\n",
    "    sy  = F.stddev(F.col(y)).over(w21)\n",
    "    return df.withColumn(out, cov / (sx * sy))\n",
    "\n",
    "# detecta retornos disponíveis\n",
    "ret_cols = [c for c in gold_ret.columns if c.endswith(\"_ret\")]\n",
    "if not ret_cols:\n",
    "    raise RuntimeError(\"[GOLD CORR ROLL] Nenhuma coluna *_ret encontrada em gold_ret.\")\n",
    "\n",
    "pairs = list(combinations(ret_cols, 2))\n",
    "\n",
    "# base mínima para rolling\n",
    "df_roll = gold_ret.select(\"trade_date\", *ret_cols).orderBy(\"trade_date\")\n",
    "\n",
    "# calcula rolling para todos os pares, gerando colunas corr__A__B__21d\n",
    "out_cols = []\n",
    "for a, b in pairs:\n",
    "    a_name = a.replace(\"_ret\", \"\")\n",
    "    b_name = b.replace(\"_ret\", \"\")\n",
    "    out = f\"corr__{a_name}__{b_name}__21d\"\n",
    "    df_roll = rolling_corr(df_roll, a, b, out)\n",
    "    out_cols.append(out)\n",
    "\n",
    "# converte para formato longo: (trade_date, asset_a, asset_b, corr_21d)\n",
    "stack_structs = []\n",
    "for out in out_cols:\n",
    "    # out = corr__A__B__21d\n",
    "    _, a_name, b_name, _ = out.split(\"__\")\n",
    "    stack_structs.append(\n",
    "        F.struct(\n",
    "            F.lit(a_name).alias(\"asset_a\"),\n",
    "            F.lit(b_name).alias(\"asset_b\"),\n",
    "            F.col(out).alias(\"corr_21d\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "corr_roll_long = (\n",
    "    df_roll\n",
    "    .select(\"trade_date\", F.explode(F.array(*stack_structs)).alias(\"x\"))\n",
    "    .select(\"trade_date\", \"x.asset_a\", \"x.asset_b\", \"x.corr_21d\")\n",
    ")\n",
    "\n",
    "display(corr_roll_long.orderBy(\"trade_date\").limit(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453acadc-cfa0-4b54-a263-dc0522214b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC #### Quality Gate — rolling corr em [-1, 1] quando não nula\n",
    "# COMMAND ----------\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "w21 = Window.orderBy(\"trade_date\").rowsBetween(-20, 0)\n",
    "\n",
    "def rolling_corr_safe(df, x, y, out, min_points=10, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Rolling corr robusta:\n",
    "    - exige um mínimo de pontos válidos (min_points) para calcular\n",
    "    - evita divisão por zero (stddev = 0 ou NULL)\n",
    "    - clampa pequenos overshoots numéricos em [-1,1]\n",
    "    \"\"\"\n",
    "    cx  = F.count(F.col(x)).over(w21)\n",
    "    cy  = F.count(F.col(y)).over(w21)\n",
    "    cxy = F.count(F.col(x) * F.col(y)).over(w21)  # conta linhas onde ambos existem\n",
    "    ex  = F.avg(F.col(x)).over(w21)\n",
    "    ey  = F.avg(F.col(y)).over(w21)\n",
    "    exy = F.avg(F.col(x) * F.col(y)).over(w21)\n",
    "    cov = exy - (ex * ey)\n",
    "    sx  = F.stddev(F.col(x)).over(w21)\n",
    "    sy  = F.stddev(F.col(y)).over(w21)\n",
    "\n",
    "    denom = sx * sy\n",
    "    raw = cov / denom\n",
    "\n",
    "    # condições de validade\n",
    "    valid = (\n",
    "        (cxy >= F.lit(min_points)) &\n",
    "        denom.isNotNull() &\n",
    "        (denom > F.lit(0))\n",
    "    )\n",
    "\n",
    "    # clamp numérico: aceita pequeno overshoot por eps\n",
    "    clamped = (\n",
    "        F.when(raw > F.lit(1) + F.lit(eps), F.lit(1))\n",
    "         .when(raw < F.lit(-1) - F.lit(eps), F.lit(-1))\n",
    "         .otherwise(raw)\n",
    "    )\n",
    "\n",
    "    return df.withColumn(out, F.when(valid, clamped).otherwise(F.lit(None)))\n",
    "    from itertools import combinations\n",
    "\n",
    "    ret_cols = [c for c in gold_ret.columns if c.endswith(\"_ret\")]\n",
    "    pairs = list(combinations(ret_cols, 2))\n",
    "    df_roll = gold_ret.select(\"trade_date\", *ret_cols).orderBy(\"trade_date\")\n",
    "    out_cols = []\n",
    "    for a, b in pairs:\n",
    "        a_name = a.replace(\"_ret\", \"\")\n",
    "        b_name = b.replace(\"_ret\", \"\")\n",
    "        out = f\"corr__{a_name}__{b_name}__21d\"\n",
    "        df_roll = rolling_corr_safe(df_roll, a, b, out, min_points=10, eps=1e-9)\n",
    "        out_cols.append(out)\n",
    "    stack_structs = []\n",
    "    for out in out_cols:\n",
    "        _, a_name, b_name, _ = out.split(\"__\")\n",
    "        stack_structs.append(\n",
    "            F.struct(\n",
    "                F.lit(a_name).alias(\"asset_a\"),\n",
    "                F.lit(b_name).alias(\"asset_b\"),\n",
    "                F.col(out).alias(\"corr_21d\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    corr_roll_long = (\n",
    "        df_roll\n",
    "        .select(\"trade_date\", F.explode(F.array(*stack_structs)).alias(\"x\"))\n",
    "        .select(\"trade_date\", \"x.asset_a\", \"x.asset_b\", \"x.corr_21d\")\n",
    "    )\n",
    "\n",
    "    display(corr_roll_long.orderBy(\"trade_date\").limit(30))\n",
    "    tol = 1e-6\n",
    "    bad_corr = (\n",
    "        corr_roll_long\n",
    "        .dropna(subset=[\"corr_21d\"])\n",
    "        .filter((F.col(\"corr_21d\") > F.lit(1 + tol)) | (F.col(\"corr_21d\") < F.lit(-1 - tol)))\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    if bad_corr > 0:\n",
    "        display(\n",
    "            corr_roll_long\n",
    "            .dropna(subset=[\"corr_21d\"])\n",
    "            .filter((F.col(\"corr_21d\") > F.lit(1 + tol)) | (F.col(\"corr_21d\") < F.lit(-1 - tol)))\n",
    "            .orderBy(\"trade_date\")\n",
    "            .limit(200)\n",
    "        )\n",
    "        raise RuntimeError(f\"[GOLD CORR ROLL] valores fora de [-1,1] além da tolerância (qtde={bad_corr})\")\n",
    "\n",
    "    print(\"[GOLD CORR ROLL] OK\")\n",
    "\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_corr_pairs_rolling_21d\")\n",
    "    corr_roll_long.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_corr_pairs_rolling_21d\")\n",
    "    print(\"✅ Tabela criada: mvp_finance.gold_corr_pairs_rolling_21d\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7831e2f-cc0a-47bf-95e1-b9d5c471ff08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Heatmap — Matriz de correlação global (retornos, multi-ativo)\n",
    "# MAGIC\n",
    "# MAGIC - Fonte: `gold_corr_pairs_global`\n",
    "# MAGIC - Correlação de Pearson no período inteiro\n",
    "# MAGIC - Funciona automaticamente para N ativos\n",
    "# COMMAND ----------\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# lê da Gold (fonte oficial)\n",
    "corr_pairs_pd = (\n",
    "    spark.table(\"gold_corr_pairs_global\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# lista completa de ativos\n",
    "assets = sorted(\n",
    "    set(corr_pairs_pd[\"asset_a\"]).union(corr_pairs_pd[\"asset_b\"])\n",
    ")\n",
    "\n",
    "# inicializa matriz identidade\n",
    "corr_matrix = pd.DataFrame(\n",
    "    data=1.0,\n",
    "    index=assets,\n",
    "    columns=assets\n",
    ")\n",
    "\n",
    "# preenche pares\n",
    "for _, row in corr_pairs_pd.iterrows():\n",
    "    a, b, v = row[\"asset_a\"], row[\"asset_b\"], row[\"corr_pearson\"]\n",
    "    corr_matrix.loc[a, b] = v\n",
    "    corr_matrix.loc[b, a] = v  # simetria\n",
    "\n",
    "# heatmap\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    text_auto=\".2f\",\n",
    "    aspect=\"auto\",\n",
    "    zmin=-1, zmax=1,\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    title=\"Matriz de Correlação Global — Retornos (Pearson)\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Ativo\",\n",
    "    yaxis_title=\"Ativo\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46f49c7-da73-42ae-b28a-7876bb43990e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5) GOLD — Tabela analítica unificada (multi-ativo) (`gold_analytics_assets`)\n",
    "# MAGIC\n",
    "# MAGIC Junta (por **ativo** e **dia**):\n",
    "# MAGIC - Features técnicas do ativo (Gold features)\n",
    "# MAGIC - Retornos/preços wide (contexto multi-ativo)\n",
    "# MAGIC - Correlações rolling 21d do ativo vs demais\n",
    "# MAGIC\n",
    "# MAGIC Resultado: dataset pronto para EDA, validação de hipóteses e ML.\n",
    "# COMMAND ----------\n",
    "spark.sql(\"USE mvp_finance\")\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from itertools import combinations\n",
    "\n",
    "def table_exists(db: str, name: str) -> bool:\n",
    "    try:\n",
    "        spark.table(f\"{db}.{name}\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "DB = \"mvp_finance\"\n",
    "\n",
    "# possíveis nomes (caso você tenha persistido com nomes antigos)\n",
    "CANDIDATES = [\n",
    "    \"gold_corr_pairs_rolling_21d\",\n",
    "    \"gold_corr_ibov_global_rolling\",\n",
    "    \"gold_corr_pairs_rolling_21d\".lower(),\n",
    "    \"gold_corr_ibov_global_rolling\".lower(),\n",
    "]\n",
    "\n",
    "rolling_table = next((t for t in CANDIDATES if table_exists(DB, t)), None)\n",
    "\n",
    "gold_ret = spark.table(f\"{DB}.gold_retornos_global\").withColumnRenamed(\"trade_date\", \"trade_date\")  # só garantindo referência\n",
    "\n",
    "if rolling_table:\n",
    "    print(f\"[OK] Usando rolling existente: {DB}.{rolling_table}\")\n",
    "    corr_roll_long = (\n",
    "        spark.table(f\"{DB}.{rolling_table}\")\n",
    "        .select(\"trade_date\", \"asset_a\", \"asset_b\", \"corr_21d\")\n",
    "    )\n",
    "else:\n",
    "    print(\"[INFO] Rolling não encontrado em tabela. Recalculando a partir de gold_retornos_global...\")\n",
    "\n",
    "    # detecta todas colunas *_ret\n",
    "    ret_cols = [c for c in gold_ret.columns if c.endswith(\"_ret\")]\n",
    "    if not ret_cols:\n",
    "        raise RuntimeError(\"[ROLLING] gold_retornos_global não possui colunas *_ret.\")\n",
    "\n",
    "    pairs = list(combinations(ret_cols, 2))\n",
    "\n",
    "    w21 = Window.orderBy(\"trade_date\").rowsBetween(-20, 0)\n",
    "\n",
    "    def rolling_corr_safe(df, x, y, out, min_points=10, eps=1e-9):\n",
    "        cxy = F.count(F.col(x) * F.col(y)).over(w21)\n",
    "        ex  = F.avg(F.col(x)).over(w21)\n",
    "        ey  = F.avg(F.col(y)).over(w21)\n",
    "        exy = F.avg(F.col(x) * F.col(y)).over(w21)\n",
    "        cov = exy - (ex * ey)\n",
    "        sx  = F.stddev(F.col(x)).over(w21)\n",
    "        sy  = F.stddev(F.col(y)).over(w21)\n",
    "        denom = sx * sy\n",
    "        raw = cov / denom\n",
    "\n",
    "        valid = (cxy >= F.lit(min_points)) & denom.isNotNull() & (denom > F.lit(0))\n",
    "\n",
    "        clamped = (\n",
    "            F.when(raw > F.lit(1) + F.lit(eps), F.lit(1))\n",
    "             .when(raw < F.lit(-1) - F.lit(eps), F.lit(-1))\n",
    "             .otherwise(raw)\n",
    "        )\n",
    "        return df.withColumn(out, F.when(valid, clamped).otherwise(F.lit(None)))\n",
    "\n",
    "    df_roll = gold_ret.select(\"trade_date\", *ret_cols).orderBy(\"trade_date\")\n",
    "\n",
    "    out_cols = []\n",
    "    for a, b in pairs:\n",
    "        a_name = a.replace(\"_ret\", \"\")\n",
    "        b_name = b.replace(\"_ret\", \"\")\n",
    "        out = f\"corr__{a_name}__{b_name}__21d\"\n",
    "        df_roll = rolling_corr_safe(df_roll, a, b, out)\n",
    "        out_cols.append(out)\n",
    "\n",
    "    # formato longo\n",
    "    stack_structs = []\n",
    "    for out in out_cols:\n",
    "        _, a_name, b_name, _ = out.split(\"__\")\n",
    "        stack_structs.append(\n",
    "            F.struct(\n",
    "                F.lit(a_name).alias(\"asset_a\"),\n",
    "                F.lit(b_name).alias(\"asset_b\"),\n",
    "                F.col(out).alias(\"corr_21d\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    corr_roll_long = (\n",
    "        df_roll\n",
    "        .select(\"trade_date\", F.explode(F.array(*stack_structs)).alias(\"x\"))\n",
    "        .select(\"trade_date\", \"x.asset_a\", \"x.asset_b\", \"x.corr_21d\")\n",
    "    )\n",
    "\n",
    "    # persiste (opcional, mas recomendado)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {DB}.gold_corr_pairs_rolling_21d\")\n",
    "    corr_roll_long.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{DB}.gold_corr_pairs_rolling_21d\")\n",
    "    print(f\"✅ Tabela criada: {DB}.gold_corr_pairs_rolling_21d\")\n",
    "\n",
    "gold_features = spark.table(\"gold_asset_features\")\n",
    "gold_ret_wide  = spark.table(\"gold_retornos_global\")\n",
    "# corr_roll_long já está definido pelo resolver acima\n",
    "ret_small = gold_ret_wide.withColumnRenamed(\"trade_date\", \"date\")\n",
    "\n",
    "corr_sym = (\n",
    "    corr_roll_long\n",
    "    .withColumnRenamed(\"trade_date\", \"date\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb96c18-a2fc-4a32-909c-21849d8ee1b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark.sql(\"USE mvp_finance\")\n",
    "\n",
    "# =========================\n",
    "# 1) Fontes Gold existentes\n",
    "# =========================\n",
    "gold_features = spark.table(\"gold_asset_features\")              # por symbol + date\n",
    "gold_ret      = spark.table(\"gold_retornos_global\")             # contexto global (wide)\n",
    "corr_roll     = spark.table(\"gold_corr_pairs_rolling_21d\")      # long: date, asset_a, asset_b, corr_21d\n",
    "\n",
    "# =========================\n",
    "# 2) Retornos (padroniza chave)\n",
    "# =========================\n",
    "ret_small = (\n",
    "    gold_ret\n",
    "    .withColumnRenamed(\"trade_date\", \"date\")\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 3) Correlação rolling → pivot por (date, symbol)\n",
    "# =========================\n",
    "corr_sym = (\n",
    "    corr_roll\n",
    "    .withColumnRenamed(\"trade_date\", \"date\")\n",
    ")\n",
    "\n",
    "corr_as_a = corr_sym.select(\n",
    "    \"date\",\n",
    "    F.col(\"asset_a\").alias(\"symbol\"),\n",
    "    F.col(\"asset_b\").alias(\"other\"),\n",
    "    \"corr_21d\"\n",
    ")\n",
    "\n",
    "corr_as_b = corr_sym.select(\n",
    "    \"date\",\n",
    "    F.col(\"asset_b\").alias(\"symbol\"),\n",
    "    F.col(\"asset_a\").alias(\"other\"),\n",
    "    \"corr_21d\"\n",
    ")\n",
    "\n",
    "corr_long = corr_as_a.unionByName(corr_as_b)\n",
    "\n",
    "corr_pivot = (\n",
    "    corr_long\n",
    "    .groupBy(\"date\", \"symbol\")\n",
    "    .pivot(\"other\")\n",
    "    .agg(F.first(\"corr_21d\"))\n",
    ")\n",
    "\n",
    "# renomeia colunas de correlação\n",
    "for c in corr_pivot.columns:\n",
    "    if c not in (\"date\", \"symbol\"):\n",
    "        corr_pivot = corr_pivot.withColumnRenamed(c, f\"corr_21d__{c}\")\n",
    "\n",
    "# =========================\n",
    "# 4) Features por ativo\n",
    "# =========================\n",
    "feat_small = gold_features.select(\n",
    "    \"symbol\",\n",
    "    \"date\",\n",
    "    \"timestamp\",\n",
    "    \"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "    \"retorno\",\"retorno_log\",\n",
    "    \"sma20\",\"sma50\",\n",
    "    \"boll_upper\",\"boll_lower\",\n",
    "    \"atr14\",\n",
    "    \"label_direcao\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5) JOIN FINAL\n",
    "# =========================\n",
    "gold_analytics_assets = (\n",
    "    feat_small.alias(\"f\")\n",
    "    .join(ret_small.alias(\"r\"), [\"date\"], \"inner\")\n",
    "    .join(corr_pivot.alias(\"c\"), [\"date\", \"symbol\"], \"left\")\n",
    ")\n",
    "\n",
    "display(gold_analytics_assets.orderBy(\"date\", \"symbol\").limit(20))\n",
    "\n",
    "# =========================\n",
    "# 6) Persistência DEFINITIVA\n",
    "# =========================\n",
    "spark.sql(\"DROP TABLE IF EXISTS mvp_finance.gold_analytics_assets\")\n",
    "\n",
    "(\n",
    "    gold_analytics_assets\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"mvp_finance.gold_analytics_assets\")\n",
    ")\n",
    "\n",
    "print(\"✅ Tabela criada: mvp_finance.gold_analytics_assets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aedb5ab1-db83-418c-9753-ea3ea9650dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_final = spark.table(\"mvp_finance.gold_analytics_assets\")\n",
    "gold_final.count()\n",
    "gold_final.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e06632f5-9d20-48bf-9449-6278e39adadb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# date e symbol não nulos\n",
    "assert gold_final.filter(F.col(\"date\").isNull()).count() == 0\n",
    "assert gold_final.filter(F.col(\"symbol\").isNull()).count() == 0\n",
    "\n",
    "# unicidade\n",
    "assert (\n",
    "    gold_final.groupBy(\"date\",\"symbol\").count().filter(\"count > 1\").count()\n",
    ") == 0\n",
    "\n",
    "print(\"[GOLD FINAL] OK — multi-ativo consistente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef1ab98-59ee-40f7-a695-e70d9317d6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.1) Quality Gate — `gold_analytics_assets` (multi-ativo) [UPGRADED]\n",
    "# MAGIC Validações:\n",
    "# MAGIC - `date` e `symbol` não nulos\n",
    "# MAGIC - sem duplicidade por (date, symbol)\n",
    "# MAGIC - colunas críticas existem\n",
    "# MAGIC - checks numéricos básicos (close/volume/label)\n",
    "# MAGIC - NULLs esperados apenas no aquecimento (por ativo)\n",
    "# COMMAND ----------\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "DB = \"mvp_finance\"\n",
    "TBL = f\"{DB}.gold_analytics_assets\"\n",
    "\n",
    "gold_final = spark.table(TBL)\n",
    "\n",
    "# 0) Normaliza date (garante que é DATE para ordenação e joins)\n",
    "# Se já for DATE, to_date mantém igual.\n",
    "gold_final = gold_final.withColumn(\"date\", F.to_date(F.col(\"date\")))\n",
    "\n",
    "# 1) date e symbol não nulos\n",
    "null_date = gold_final.filter(F.col(\"date\").isNull()).count()\n",
    "if null_date > 0:\n",
    "    raise RuntimeError(f\"[GOLD FINAL] date NULL (qtde={null_date})\")\n",
    "\n",
    "null_symbol = gold_final.filter(F.col(\"symbol\").isNull()).count()\n",
    "if null_symbol > 0:\n",
    "    raise RuntimeError(f\"[GOLD FINAL] symbol NULL (qtde={null_symbol})\")\n",
    "\n",
    "# 2) unicidade por (date, symbol)\n",
    "dups_key = (\n",
    "    gold_final\n",
    "    .groupBy(\"date\", \"symbol\")\n",
    "    .count()\n",
    "    .filter(F.col(\"count\") > 1)\n",
    "    .count()\n",
    ")\n",
    "if dups_key > 0:\n",
    "    display(\n",
    "        gold_final.groupBy(\"date\",\"symbol\").count().filter(F.col(\"count\") > 1)\n",
    "                 .orderBy(F.col(\"count\").desc(), \"date\", \"symbol\")\n",
    "                 .limit(200)\n",
    "    )\n",
    "    raise RuntimeError(f\"[GOLD FINAL] duplicidade por (date, symbol) (qtde={dups_key})\")\n",
    "\n",
    "# 3) colunas obrigatórias (mínimo de contrato)\n",
    "required_cols = [\n",
    "    \"date\", \"symbol\",\n",
    "    \"close\", \"retorno\", \"label_direcao\"\n",
    "]\n",
    "missing = [c for c in required_cols if c not in gold_final.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[GOLD FINAL] colunas obrigatórias ausentes: {missing}\")\n",
    "\n",
    "# 4) sanity checks numéricos básicos\n",
    "# - close deve ser > 0 (para índices/ativos típicos; se houver algum que possa ser 0, ajuste)\n",
    "bad_close = gold_final.filter(F.col(\"close\").isNull() | (F.col(\"close\") <= 0)).count()\n",
    "if bad_close > 0:\n",
    "    display(gold_final.filter(F.col(\"close\").isNull() | (F.col(\"close\") <= 0))\n",
    "                 .select(\"date\",\"symbol\",\"close\").orderBy(\"date\",\"symbol\").limit(200))\n",
    "    raise RuntimeError(f\"[GOLD FINAL] close inválido (NULL ou <=0) (qtde={bad_close})\")\n",
    "\n",
    "# - volume deve ser >= 0 (se algum ativo não tiver volume, pode ficar NULL — aqui não forçamos)\n",
    "if \"volume\" in gold_final.columns:\n",
    "    bad_vol = gold_final.filter(F.col(\"volume\").isNotNull() & (F.col(\"volume\") < 0)).count()\n",
    "    if bad_vol > 0:\n",
    "        display(gold_final.filter(F.col(\"volume\").isNotNull() & (F.col(\"volume\") < 0))\n",
    "                     .select(\"date\",\"symbol\",\"volume\").orderBy(\"date\",\"symbol\").limit(200))\n",
    "        raise RuntimeError(f\"[GOLD FINAL] volume negativo (qtde={bad_vol})\")\n",
    "\n",
    "# - label deve ser 0/1\n",
    "bad_label = gold_final.filter(~F.col(\"label_direcao\").isin(0, 1)).count()\n",
    "if bad_label > 0:\n",
    "    display(gold_final.filter(~F.col(\"label_direcao\").isin(0,1))\n",
    "                 .select(\"date\",\"symbol\",\"retorno\",\"label_direcao\").orderBy(\"date\",\"symbol\").limit(200))\n",
    "    raise RuntimeError(f\"[GOLD FINAL] label_direcao fora de {{0,1}} (qtde={bad_label})\")\n",
    "\n",
    "# - retorno não pode ser NaN/Inf quando não nulo\n",
    "bad_ret = gold_final.filter(\n",
    "    F.col(\"retorno\").isNotNull() & (F.isnan(\"retorno\") | F.col(\"retorno\").isin(float(\"inf\"), float(\"-inf\")))\n",
    ").count()\n",
    "if bad_ret > 0:\n",
    "    display(gold_final.filter(\n",
    "        F.col(\"retorno\").isNotNull() & (F.isnan(\"retorno\") | F.col(\"retorno\").isin(float(\"inf\"), float(\"-inf\")))\n",
    "    ).select(\"date\",\"symbol\",\"close\",\"retorno\").orderBy(\"date\",\"symbol\").limit(200))\n",
    "    raise RuntimeError(f\"[GOLD FINAL] retorno NaN/Inf (qtde={bad_ret})\")\n",
    "\n",
    "# 5) aquecimento por ativo (depois disso esperamos retorno e label presentes)\n",
    "W = Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "N_WARMUP = 60  # cobre SMA50 + Boll20 + ATR14 com folga\n",
    "\n",
    "qc = (\n",
    "    gold_final\n",
    "    .withColumn(\"_rn\", F.row_number().over(W))\n",
    "    .filter(F.col(\"_rn\") > F.lit(N_WARMUP))\n",
    ")\n",
    "\n",
    "critical_after = [\"retorno\", \"label_direcao\"]\n",
    "\n",
    "null_post = qc.select(\n",
    "    *[F.sum(F.col(c).isNull().cast(\"int\")).alias(f\"null_{c}\") for c in critical_after]\n",
    ").collect()[0].asDict()\n",
    "\n",
    "bad_total = int(sum(null_post.values()))\n",
    "if bad_total > 0:\n",
    "    print(\"[GOLD FINAL] NULLs pós-aquecimento:\", null_post)\n",
    "    display(\n",
    "        qc.filter(F.col(\"retorno\").isNull() | F.col(\"label_direcao\").isNull())\n",
    "          .select(\"date\",\"symbol\",\"close\",\"retorno\",\"label_direcao\")\n",
    "          .orderBy(\"date\",\"symbol\")\n",
    "          .limit(200)\n",
    "    )\n",
    "    raise RuntimeError(f\"[GOLD FINAL] NULLs inesperados pós-aquecimento (total={bad_total})\")\n",
    "\n",
    "print(\"[GOLD FINAL] OK ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02402bb-7aef-49f7-82ce-c997235e1d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.2) Persistência — `gold_analytics_ibov_global`\n",
    "# COMMAND ----------\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold_analytics_ibov_global\")\n",
    "\n",
    "gold_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_analytics_ibov_global\")\n",
    "\n",
    "print(\"✅ Tabela criada: mvp_finance.gold_analytics_ibov_global\")\n",
    "spark.sql(\"SELECT COUNT(*) AS qtde FROM gold_analytics_ibov_global\").show()\n",
    "spark.sql(\"SELECT * FROM gold_analytics_ibov_global ORDER BY date LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0405f102-7c4c-4df8-a55f-f05138a4f58f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusão — Camada Gold\n",
    "\n",
    "A camada **Gold** representa a **materialização analítica final** do pipeline, transformando dados padronizados da Silver em **informação pronta para decisão, pesquisa e modelagem preditiva**.\n",
    "\n",
    "### Síntese do que foi construído\n",
    "Nesta camada foram consolidados, de forma sistemática e escalável:\n",
    "\n",
    "- **Features técnicas por ativo**, calculadas de forma independente e causal:\n",
    "\n",
    "    - retornos simples e logarítmicos\n",
    "    - médias móveis (SMA 20 / 50)\n",
    "    - Bandas de Bollinger\n",
    "    - True Range e ATR\n",
    "    - labels direcionais para classificação\n",
    "\n",
    "- **Retornos globais multi-ativo**, baseados em séries estacionárias, garantindo:\n",
    "\n",
    "    - comparabilidade entre mercados\n",
    "    - robustez estatística\n",
    "    - adequação para correlação e Machine Learning\n",
    "\n",
    "- **Correlação global (Pearson)** entre todos os pares de ativos, fornecendo:\n",
    "\n",
    "    - visão estrutural do relacionamento entre mercados\n",
    "    - base para seleção e redução de variáveis\n",
    "\n",
    "- **Correlação dinâmica (rolling 21 períodos)**, permitindo:\n",
    "\n",
    "    - identificação de mudanças de regime\n",
    "    - análise temporal da força e direção das relações\n",
    "    - diferenciação entre correlação estrutural e circunstancial\n",
    "\n",
    "- **Dataset analítico multi-ativo**, integrando:\n",
    "\n",
    "    - features técnicas do ativo\n",
    "    - contexto global diário\n",
    "    - correlações dinâmicas do ativo versus os demais\n",
    "\n",
    "### Qualidade, rigor e governança\n",
    "\n",
    "A camada Gold foi construída com **preocupação explícita com rigor estatístico**, incluindo:\n",
    "\n",
    "- uso exclusivo de **retornos** para correlação (evitando não-estacionariedade)\n",
    "- proteção contra divisões por zero e variância nula em janelas rolling\n",
    "- Quality Gates para:\n",
    "- chaves temporais\n",
    "- unicidade por `(date, symbol)`\n",
    "- limites matemáticos das correlações\n",
    "\n",
    "Valores ausentes são tratados como **informação válida** apenas:\n",
    "\n",
    "- durante o aquecimento inicial das janelas\n",
    "- quando a estatística não é definida (ex.: variância zero)\n",
    "\n",
    "### Resultado final\n",
    "Ao final da camada Gold, o pipeline entrega um **ativo de dados de alto valor**, que:\n",
    "\n",
    "- está pronto para **EDA avançada**\n",
    "- suporta **validação formal de hipóteses**\n",
    "- pode ser consumido diretamente por modelos de **Machine Learning**\n",
    "- permite evolução para **sinais operacionais e backtests**\n",
    "\n",
    "Esta camada encerra o ciclo de transformação de dados e inaugura o ciclo de **inteligência e tomada de decisão**, mantendo separação clara entre:\n",
    "    - engenharia de dados (Bronze / Silver)\n",
    "    - análise, modelagem e estratégia (Gold)\n",
    "\n",
    "O pipeline, como um todo, encontra-se **coerente, escalável e cientificamente sólido**, pronto para uso em ambientes de pesquisa aplicada ou produção analítica.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4748028691104431,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_gold_features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
